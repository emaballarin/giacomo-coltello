---
title: "SMDS Homework - Block 3"
author: "R. Corti, L. Taroni, J.A. Fernandez Santisteban and E. Ballarin  |  Group 'D'"
date: "13th May 2020"
output:
  html_document:
    theme: darkly
    highlight: breezedark
    mathjax: default
    self_contained: true
    md_extensions: +autolink_bare_uris
    toc: true
    toc_collapsed: false
    toc_float: true
    toc_depth: 3
    number_sections: false
header-includes:
- \usepackage{color}
- \usepackage{graphicx}
- \usepackage{grffile}
institute: University of Trieste, SISSA, ICTP, University of Udine
graphics: yes
fontsize: 10pt
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.align = 'center', warning=FALSE, message=FALSE, fig.asp=0.625, dev='png', global.par = TRUE, dev.args=list(pointsize=10), fig.path = 'figs/')
```

```{r setup, include=FALSE}
library(knitr)
local({
  hook_plot = knit_hooks$get('plot')
  knit_hooks$set(plot = function(x, options) {
    paste0('\n\n----\n\n', hook_plot(x, options))
  })
})
```

# Exercises from *LEC*

## Exercise 1

### Text:

Compute the *bootstrap*-based confidence interval for the $\mathsf{score}$ dataset using the *studentized* method.

### Solution:

```{r lec_01, code = readLines("src/lec_01.R"), echo=TRUE}
```

### Comment:

The exercise has been solved by estimating (since required and not explicitly given) *standard errors* for the statistic of interest in each *bootstrap* resample via another *bootstrap* iteration. This schema is known as *bootstrap-in-bootstrap*; similarly *jackknife-in-bootstrap* provides an equally viable (though linearly-approximated) solution to the problem.

## Exercise 2

### Text:

Compute *bootstrap*-based confidence intervals for the $\mathsf{score}$ dataset using the $\mathsf{boot}$ package.

### Solution:

```{r lec_02, code = readLines("src/lec_02.R"), echo=TRUE}
```

### Comment:

As for the previous exercise, also in this case *bootstrap-in-bootstrap* has been used. All *bootstrap* iterations are implemented by using the $\mathsf{boot}$ package.


# Exercises from *LAB*

## Exercise 1

## Exercise 2

### Text:

The Wald confidence interval with level $1−\alpha$ is defined as:
$$ \hat{\gamma} \pm z_{1-\alpha/2}j_{P}(\hat{\gamma})^{-1/2}.$$

Compute the Wald confidence interval of level 0.95 and plot the results.

### Solution:

In order to have the profile likelihood, let's consider the vector $y$ and the log-likelihood of the Weibull Model $l(\gamma, \beta)$:
```{r echo=TRUE,  message=FALSE, warning=FALSE}

y <- c(155.9, 200.2, 143.8, 150.1,152.1, 142.2, 147, 146, 146,
       170.3, 148, 140, 118, 144, 97)
n <- length(y)

log_lik_weibull <- function( data, param){
  -sum(dweibull(data, shape = param[1], scale = param[2], log = TRUE))
}
```


Given this function, the profile likelihood $l_P(\gamma)$ is built evaluating the log-likelihood to $y, \gamma$ and  $\hat{\beta}_{\gamma}= (\sum_{i=1}^{n} y^{\gamma}_{i}/n)^{1/\gamma}$:

```{r echo=TRUE,  message=FALSE, warning=FALSE}
log_lik_weibull_profile_gamma  <- function(data, gamma){
  beta.gamma <- mean(data^gamma)^(1/gamma)
  log_lik_weibull( data, c(gamma, beta.gamma) )
}
```

After having defined the above function, we find the MLE $\hat{\gamma}$ using the $\mathsf{optim()}$ numerical method specifying `hessian=T`. Then, the Wald interval will be defined as $\hat{\gamma} \pm z_{1-\alpha/2}j_{P}(\hat{\gamma})^{-1/2}$, where $j_{P}(\hat{\gamma})$ is the hessian evaluated at $\gamma = \hat{\gamma}$:
```{r echo=TRUE,  message=FALSE, warning=FALSE}
weib_mle<-optim(1 ,fn=log_lik_weibull_profile_gamma,hessian=T,
                  method='L-BFGS-B',lower=rep(1e-7,2),
                  upper=rep(Inf,2),data=y)
weib_mle$par

weib_mle_se<-(weib_mle$hessian[1,1])^(-1/2)
conf_level <- 0.95
weib_mle_ci <- weib_mle$par + c(-1,1)* weib_mle_se * qnorm(1-(1-conf_level)/2)
weib_mle_ci
```

```{r echo=TRUE,  message=FALSE, warning=FALSE}
log_lik_weibull_profile_gamma_v <-Vectorize(log_lik_weibull_profile_gamma, 'gamma'  )
plot(function(x) -log_lik_weibull_profile_gamma_v(data=y, x)+weib_mle$value,
     from=0.1,to=15,xlab=expression(gamma),
     ylab='profile relative log likelihood',
     ylim=c(-8,0))


segments(x0=weib_mle_ci[1], y0=-log_lik_weibull_profile_gamma_v(y,weib_mle_ci[1])+weib_mle$value,
         x1= weib_mle_ci[1], y1=-10,  col="green", lty=2)

segments(x0=weib_mle_ci[2], y0=-log_lik_weibull_profile_gamma_v(y,weib_mle_ci[2])+weib_mle$value, x1= weib_mle_ci[2], y1=-10,  col="green", lty=2)

abline(v=weib_mle$par[1], col="green", lwd=1, lty=1)
text(10, -1, "MLE for Gamma", col="green")
text(7,-7,"95% Wald CI",col="green")
segments( weib_mle_ci[1], -6.7, weib_mle_ci[2], -6.7, col="green", lty =1, lwd=2  )
```

## Exercise 3

### Text:

Repeat the steps above —write the profile log-likelihood, plot it and find the deviance confidence intervals— considering this time $\gamma$ as a nuisance parameter and $\beta$ as the parameter of interest.

### Solution:

In order to evaluate the profile log-likelihood for $\gamma$ as a nuisance parameter and $\beta$ as the parameter of interest we noticed that $\gamma$ is note explicitly expressed in terms of $\beta$. We used the numerical method $\mathsf{uniroot()}$  in order to fix $\gamma = \hat{\gamma}$, since we know that this estimator satisfies for every $\beta$:

$$ \frac{n}{\hat{\gamma}} - n \log(\beta) + \sum_i(\log(y_i)) - \sum_i\Bigg[\bigg(\frac{y_i}{\beta}\bigg)^{\hat{\gamma}} \log\bigg(\frac{y_i}{\beta}\bigg) \Bigg] = 0 $$
Subsequently we proceed to compute the deviance confidence intervals with level $1-\alpha$ as:

$$\{\beta: \ell_P(\beta) \geq \ell_P(\hat \beta) - \frac{1}{2}\chi^2_{1;1-\alpha}\}$$
```{r echo=TRUE,  message=FALSE, warning=FALSE}
gamma <- seq(0.1, 15, length=100)
beta <- seq(100,200, length=100)

weib_mle<-optim(c(1,1),fn=log_lik_weibull,hessian=T,
                  method='L-BFGS-B',lower=rep(1e-7,2),
                  upper=rep(Inf,2),data=y)

log_lik_weibull_profile_beta <- function(data, beta) {
  gamma.beta <- uniroot(function(x) n/x - n * log(beta) + sum(log(data)) - sum((data/beta)^x * log(data/beta)), c(1e-5,15))$root
  log_lik_weibull(data, c(gamma.beta, beta))
}
```


```{r echo=TRUE,  message=FALSE, warning=FALSE}
log_lik_weibull_profile_beta_vec <-Vectorize(log_lik_weibull_profile_beta, 'beta')

plot(function(x) -log_lik_weibull_profile_beta_vec(data=y, x) + weib_mle$value, from=120,to=200,
     xlab=expression(beta),
     ylab='profile relative log likelihood',
     ylim=c(-10,0))

conf.level<-0.95
lrt.ci1<-uniroot(function(x) -log_lik_weibull_profile_beta(data = y, x)+
                   weib_mle$value+
                   qchisq(conf.level,1)/2,
                 c(1e-7,weib_mle$par[2]))$root


lrt.ci1<-c(lrt.ci1,uniroot(function(x) -log_lik_weibull_profile_beta(y,x)+
                             weib_mle$value+
                             qchisq(conf.level,1)/2,
                           c(weib_mle$par[2],200))$root)

abline(h=-qchisq(conf.level,1)/2,lty='dashed',col=2)
segments( lrt.ci1[1],-qchisq(conf.level,1)/2, lrt.ci1[1], 
          -log_lik_weibull_profile_beta_vec(y, lrt.ci1[1]), col="red", lty=2  )
segments( lrt.ci1[2],-qchisq(conf.level,1)/2, lrt.ci1[2],
          -log_lik_weibull_profile_beta_vec(y, lrt.ci1[2]), col="red", lty=2  )
points(lrt.ci1[1], -qchisq(0.95,1)/2, pch=16, col=2, cex=1.5)
points(lrt.ci1[2], -qchisq(0.95,1)/2, pch=16, col=2, cex=1.5)
segments( lrt.ci1[1],
          -8.1, lrt.ci1[2],
          -8.1, col="red", lty =1, lwd=2  )
text(155,-7.5,"95% Deviance CI",col=2)
```

## Exercise 5

### Text:

In **sim** in the code avove you find the MCMC output which allows to approximate the posterior distribution of our parameter of interest with $S$ draws of $\theta$. Please, produce an histogram for these random draws $\theta^{(1)},...,\theta^{(S)}$, compute the empirical quantiles, and overlap the true posterior distribution.

### Solution:

In this model, as said in the lab, the analytical form of the posterior distribution is known. In order to compare the true posterior and the simulated posterior, It has been plotted an histogram with the simulated posterior distribution and a curve for the theoretical one. The histogram has been normalized. Additionally, it are shown the 25, 50 and 75 quantiles, both theoretical and simulated.

```{r}
#EXERCISE 5
library(rstan)

# Make Stan go parallel!
options(mc.cores = parallel::detectCores())

# Avoid losing precious time
rstan::rstan_options(auto_write = TRUE)

#true mean
theta_sample <- 2
#likelihood variance
sigma2 <- 2
#sample size
n <- 10
#prior mean
mu <- 7
#prior variance
tau2 <- 2

#generate some data
set.seed(123)
y <- rnorm(n,theta_sample, sqrt(sigma2))

#posterior mean
mu_star <- ((1/tau2)*mu+(n/sigma2)*mean(y))/( (1/tau2)+(n/sigma2))
#posterior standard deviation
sd_star <- sqrt(1/( (1/tau2)+(n/sigma2)))

#launch Stan model
data<- list(N=n, y=y, sigma =sqrt(sigma2), mu = mu, tau = sqrt(tau2))
fit <- rstan::stan(file="./src/normal.stan", data = data, chains = 4, iter=2000);

#extract Stan output
sim <- rstan::extract(fit);
sample_theta <- sim[["theta"]]
emp_q <- quantile(sample_theta, probs = c(0.25, 0.5, 0.75))
th_q <- qnorm(p = c(0.25, 0.5, 0.75), mu_star, sd_star)

#plot
par(mfrow=c(1,1), pty ="m", oma=c(0,0,0,0))
hist(sim[["theta"]], probability = TRUE, ylim=c(0,1.0), 
     col="darkmagenta", border="white",
     xlab=expression(theta), ylab="Probability", main="Theoretical and simulated dist. with quantiles")
curve(dnorm(x, mu_star, sd_star), col="dodgerblue", lwd=2,
      cex.lab=2, add=T) 
abline(v=emp_q, col="maroon", lwd=2, lty=4)
abline(v=th_q, col="dodgerblue", lwd=2, lty=4)
text((emp_q[1]+th_q[1])/2, 1.0, "q25")
text((emp_q[2]+th_q[2])/2, 1.0, "q50")
text((emp_q[3]+th_q[3])/2, 1.0, "q75")
legend(0.7, 1, c("Simulated", "Theoretical"), 
       c("darkmagenta", "dodgerblue"), lwd=c(2,2), lty=c(4,4), cex=0.8)
```

As can be grafically observed the distributions are quite similar. Also the few quantiles shown in this plot. In a bit deeper diagnostic test, a qq-plot can be performed in order to verify the global behavior of the quantiles.
```{r}
#qqplot

n <- 100
p <- (1:n)/n

q_theor <- qnorm(p = p, mu_star, sd_star)
q_empir <- quantile(sample_theta, probs = p)

par(mfrow=c(1,1), pty ="m", oma=c(0,0,0,0))
plot(x=q_theor,
     y=q_empir,
     type="p",
     col="black", 
     main = "Normal q-q plot",
     xlab="Theoretical quantiles",
     ylab="Empirical quantiles")
segments(x0=0, y0=0, x1=5, y1=5, col="red") 
```

The fit to the theoretical line seems quite good. Also in the extremes where, usually, models tend to fail because of the absence of data.

## Exercise 6

### Text:

Launch the following line of $\mathsf{R}$ code:

```
posterior <- as.array(fit)
```

Use now the $\mathsf{bayesplot} package. Read the help and produce for this example, using the object posterior, the following plots:

- posterior intervals;
- posterior areas;
- marginal posterior distributions for the parameters.

Quickly comment.


### Solution:

```{r lab_06, code = readLines("src/lab_06.R"), echo=TRUE}
```

### Comment:

XYZ

## Exercise 7

### Text:

Suppose you receive $n=15$ phone calls in a day, and you want to build a model to assess their average length. Your likelihood for each call length is $y_i \sim \mathrm{Poisson}(\lambda)$. Now, you have to choose the prior $\pi(\lambda)$. Please, tell which of these priors is adequate to describe the problem, and provide a short motivation for each of them:

1. $\pi(\lambda)=\mathrm{Beta}(4, 2)$;
2. $\pi(\lambda)=\mathrm{Normal}(1, 2)$;
3. $\pi(\lambda)=\mathrm{Gamma}(4, 2)$;

Now, compute your posterior as $\pi(\lambda|y) \propto L(\lambda;y)\pi(\lambda)$  for the selected prior. If your first choice was correct, you will be able to compute it analitically.

### Solution:

First of all, let's analyse the different proposed distributions. Let's use a plot to see in a clear way the main features of these distributions.

```{r}
par(mfrow=c(2,2), pty ="m", oma=c(0,0,0,0))
curve(dbeta(x, 4, 2), col="dodgerblue", lwd=2, main="Beta distribution") # x compresa fra 0 e 1 BAD
curve(dnorm(x, 1, 2), col="dodgerblue", lwd=2, xlim=c(-3,5), main="Normal distribution") # x con valori negativi BAD
curve(dgamma(x, 4, 2), col="dodgerblue", lwd=2, xlim=c(0,5), main="Gamma distribution") # x con valori positivi e non necessariamente ristretti GOOD

```

It is important to note that the model must simulate lengths of calls, which is a measurement of time (a real value always greater than 0). The first proposed model is a $\mathrm{Poisson}(\lambda)$ distribution in which $\lambda$ is both the mean and the variance of the distribution, in this case, the mean on the length of the calls. The first proposed distribution is the $\mathrm{Beta}(4, 2)$, corresponding to the first plot. This distribution confines the $\lambda$ between 0 and 1. It is not a realistic behavior as a phone call can have a length greater than 1, so it is not much reliable as prior assumption. The second proposed prior is a normal. A normal is able to return negative values of $\lambda$ that, actually, does not make sense in the context of a time measurement. The only suitable distribution as prior of $\lambda$ is the Gamma one. A gamma returns positive (non zero) samples of $\lambda$, which corresponds exacly with its natural domain.

Posterior can be computed with $\mathsf{stan}$. The first step is to program the model:

```{stan output.var = 'foo_jose'}
data{
  int N;
  int y[N];
  real<lower=0> alpha;
  real<lower=0> beta;
}
parameters{
  real<lower=0> lambda;
}
model{
  target += poisson_lpmf( y | lambda );
  target += gamma_lpdf(lambda|alpha, beta);
}
```
with the model ready, posterior can be computed. In order to compute the model, 15 samples have been generated by using a $\mathrm{Poisson}(\lambda=4)$.

```{r}
#simulate the data
set.seed(123)
n <- 15
true_mean <- 4
y <- rpois(n,true_mean)

#prior params
alpha<-4
beta<-2

#launch stan model
data<- list(N=n, y=y, alpha=alpha, beta=beta)
fit2 <- rstan::stan(file="./src/gamma_lambda.stan", data = data, chains = 4, iter=2000, refresh=-1);

#extract Stan output
sim <- rstan::extract(fit2)
sample_lambda <- sim[["lambda"]]

#posterior params
alpha_star = alpha + length(y)*mean(y)
beta_star = beta+length(y)

#plot
par(mfrow=c(1,1), pty ="m", oma=c(0,0,0,0))
hist(sim[["lambda"]], probability = TRUE, xlim=c(0,7), ylim=c(0,1.0), breaks=15, 
     col="darkmagenta", border="white",
     xlab=expression(lambda), ylab="Probability", main="Prior and posterior distribution")
curve(dgamma(x, 4, 2), col="dodgerblue", lwd=2,
      cex.lab=2, add=T) 
curve(dgamma(x, alpha_star, beta_star), col="maroon", lwd=2,
      cex.lab=2, add=T)
abline(v=true_mean, col="darkgoldenrod1", lwd=3, lty=1)
legend(0, 1, c("Posterior", "Th. Posterior", "Prior", expression(paste("True ", lambda))), 
       c("darkmagenta", "maroon", "dodgerblue", "darkgoldenrod1"), cex=0.8)
```

In the posterior distribution it is observed a maximum near to the true value of $\lambda$. The particular prior distribution and likelihood used in this simulation permit to compute also analitically the theoretical posterior distribution. As will be shown, the prior is the conjugate of the posterior. In the computation, multiplicative constants will be avoided in order to clarify the explanation. The first step is to write the likelihood and the prior a more friendly way:

$$
p(\mathbf{y}|\lambda)  =  \prod_{i=1}^N p(y_i|\lambda)  
              =  \prod_{i=1}^N \frac{e^{-\lambda} \lambda^{y_i}}{y_i!} 
              \propto e^{-N\lambda}\lambda^{N \bar{y}}; \ \ \ \
p(\lambda)  =  \frac{\beta^\alpha \lambda^{\alpha-1}  e^{-\beta \lambda}}{\Gamma(\alpha)}
            \propto  \lambda^{\alpha-1} e^{-\beta \lambda}
$$

Now both (simplified) expressions are multiplied.


$$
p(\lambda | \mathbf{y}) \ = \ p(\mathbf{y} | \lambda)p(\lambda)
                        \ \propto \ \lambda^{\alpha-1} e^{-\beta \lambda} e^{-N \lambda} \lambda^{N\bar{y}}
                         =  \lambda^{\alpha-1+N \bar{y}} e^{-\lambda (\beta + N)}
$$
In this formula, constants can be renamed as follows:
$$
\begin{align}
\alpha' &= \alpha + N \bar{y} \\
\beta' &= \beta + N
\end{align}
$$
The resulting formula then is the next:
$$
p(\lambda | \mathbf{y}) \ \propto \ \lambda^{\alpha'-1} e^{-\lambda \beta'} 
$$
This expression corresponds to the functional form of a $\mathrm{Gamma}(\alpha', \beta')$ distribution except for a multiplicative constant, which can be also seen as a normalization constant. Since the quantity $p(\lambda | \mathbf{y})$ must be necessarily a probability distribution, the only possible choice for the unknown normalization constant is to be the one of the Gamma distribution, so the complete posterior distribution is the next:

$$
p(\lambda | \mathbf{y}) = \mathrm{Gamma}(\alpha', \beta'); \ \ \ \ 
\alpha' = \alpha + N \bar{y}, \ \ \beta' = \beta + N
$$

Since it is known the analytical expression of the posterior, is easy to check the goodness of the Markov samples with a qqplot:

```{r}
#qqplot
n <- 100
p <- (1:n)/n

q_theor <- qgamma(p = p, alpha_star, beta_star)
q_empir <- quantile(sample_lambda, probs = p)

par(mfrow=c(1,1), pty ="m", oma=c(0,0,0,0))
plot(x=q_theor,
     y=q_empir,
     type="p",
     col="black", 
     main = "Gamma q-q plot",
     xlab="Theoretical quantiles",
     ylab="Empirical quantiles")
segments(x0=0, y0=0, x1=5.5, y1=5.5, col="red") 

```

The Markov process samples fits well in the qqplot, in a similar way as samples of the exercise 5.

## Exercise 8

### Text:

Go to this link: [rstan](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started), and follow the instructions to download and install the rstan library. Once you did it succesfully, open the file model called biparametric.stan, and replace the line

$$ \mathsf{ target+=cauchy\_lpdf(sigma| 0, 2.5);} $$ 
**with the following one:**

$$ \mathsf{ target+=uniform_lpdf(sigma|0.1,10);} $$
Which prior are you now assuming for your parameter $\sigma$? Reproduce the same plots as above and briefly comment.

### Solution:

The `biparametric.stan` file used has the following structure:
```{stan output.var = 'foo_rob'}
data{
  int N;
  real y[N];
  real a;
  real b;
}
parameters{
  real theta;
  real<lower=0> sigma;
}
model{
  target+=normal_lpdf(y|theta, sigma);
  target+=uniform_lpdf(theta|a, b );
  target+=uniform_lpdf(sigma|0.1,10);
}
```
In this case we're assuming that the parameter $\sigma$ has a prior distribution that is Uniform in the interval $[0.1,10]$
```{r echo=TRUE,  message=FALSE, warning=FALSE}
#true mean
theta_sample <- 2
#likelihood variance
sigma2 <- 2
#sample size
n <- 10

#generate some data
set.seed(123)
y <- rnorm(n,theta_sample, sqrt(sigma2))
```

```{r echo=TRUE,  message=FALSE, warning=FALSE}
library(rstan)
library(bayesplot)
library(rstanarm)
library(ggplot2)

# Make Stan go parallel!
options(mc.cores = parallel::detectCores())

# Avoid losing precious time
rstan::rstan_options(auto_write = TRUE)


data3<- list(N=n, y=y, a=-10, b=10)
fit3 <- rstan::stan(file="./src/biparametric.stan", data = data3, chains = 4, iter=2000,
             refresh=-1)
#extract stan output for biparametric model
sim3 <- rstan::extract(fit3)
posterior_biv <- as.matrix(fit3)
theta_est <- mean(sim3$theta)
sigma_est <- mean(sim3$sigma)
#mean of the samples
c(theta_est, sigma_est)
```

Once we have the MCMC samples, we represent the result with `mcmc_areas`: 
```{r echo=TRUE,  message=FALSE, warning=FALSE}
plot_title <- ggplot2::ggtitle("Posterior distributions", "with medians and 80% intervals")
bayesplot::mcmc_areas(posterior_biv, pars = c("theta","sigma"), prob = 0.8) + plot_title
```

From the result of the MCMC sampling we observe that mean and variance results for both $\theta$ and $\sigma$ are slightly different from the case in wich $\sigma \sim \text{Cauchy}^+ (0,2.5)$. This result suggests that priors used don't provide much information to the posterior distributions.

## Exercise 9

## Exercise 10

---
title: "SMDS Homework - Block 3"
author: "R. Corti, L. Taroni, J.A. Fernandez Santisteban and E. Ballarin  |  Group 'D'"
date: "13th May 2020"
output:
  html_document:
    theme: darkly
    highlight: breezedark
    mathjax: default
    self_contained: true
    md_extensions: +autolink_bare_uris
    toc: true
    toc_collapsed: false
    toc_float: true
    toc_depth: 3
    number_sections: false
header-includes:
- \usepackage{color}
- \usepackage{graphicx}
- \usepackage{grffile}
institute: University of Trieste, SISSA, ICTP, University of Udine
graphics: yes
fontsize: 10pt
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.align = 'center', warning=FALSE, message=FALSE, fig.asp=0.625, dev='png', global.par = TRUE, dev.args=list(pointsize=10), fig.path = 'figs/')
```

```{r setup, include=FALSE}
library(knitr)
local({
  hook_plot = knit_hooks$get('plot')
  knit_hooks$set(plot = function(x, options) {
    paste0('\n\n----\n\n', hook_plot(x, options))
  })
})
```

# Exercises from *LEC*

## Exercise 1

### Text:

Compute the *bootstrap*-based confidence interval for the $\mathsf{score}$ dataset using the *studentized* method.

### Solution:

```{r lec_01, code = readLines("src/lec_01.R"), echo=TRUE}
```

### Comment:

The exercise has been solved by estimating (since required and not explicitly given) *standard errors* for the statistic of interest in each *bootstrap* resample via another *bootstrap* iteration. This schema is known as *bootstrap-in-bootstrap*; similarly *jackknife-in-bootstrap* provides an equally viable (though linearly-approximated) solution to that problem.

Parallelization of independent *bootstrap* iterations is possible via the useage of the $\mathsf{doParallel}$ package, but has not been pursued due to potential compatibility breakage with R 4.0.0 *Arbor Day* (which has been used).

## Exercise 2

### Text:

Compute *bootstrap*-based confidence intervals for the $\mathsf{score}$ dataset using the $\mathsf{boot}$ package.

### Solution:

```{r lec_02, code = readLines("src/lec_02.R"), echo=TRUE}
```

### Comment:

As for the previous exercise, also in this case *bootstrap-in-bootstrap* has been used. All *bootstrap* iterations are implemented by using the $\mathsf{boot}$ package.

Parallelization of independent *bootstrap* iterations is possible via the usage of the package-integrated option `parallel = c("multicore")`, but has not been pursued due to potential compatibility breakage with R 4.0.0 *Arbor Day* (which has been used).


# Exercises from *LAB*

## Exercise 1

### Text:

Use $\mathbf{\mathsf{nml}}$  to compute the variance for the estimator $\hat{\omega}=(\log(\hat{\gamma}),\log(\hat{\beta}))$ and $\mathbf{\mathsf{optimHess}}$ for the variance $\hat{\theta}=(\hat{\gamma},\hat{\beta})$.

### Solution:

The estimate of $\hat{\omega}$ using $\mathbf{\mathsf{nml}}$ can be made simply passing the reparametrization in the imput of $\mathbf{\mathsf{log\_lik\_weibull}}$. On this way, $\mathbf{\mathsf{nml}}$ does not "know" about the existence of $\hat{\theta}$ but only of $\hat{\omega}$, so computations are made in terms of reparametrized parameters, including the hessian matrix. At this point, the procedure is to specity the computation of the hessian ($\mathbf{\mathsf{hessian=T}}$), extract the hessian matrix from the estimation and finally compute the inverse matrix of that, which contains the variance of $\hat{\omega}$.

```{r}
y <- c(155.9, 200.2, 143.8, 150.1,152.1, 142.2, 147, 146, 146,
       170.3, 148, 140, 118, 144, 97)
n <- length(y)

# log-likelihood function
log_lik_weibull <- function( data, param){
   -sum(dweibull(data, shape = param[1], scale = param[2], log = TRUE))
}

theta <- function(omega) exp(omega)
log_lik_weibull_rep <- function(data, param) log_lik_weibull(data, theta(param))
weib.y.nlm<-nlm(log_lik_weibull_rep,c(0,0),hessian=T,data=y)
weib.y.nlm

#variances from information matrix of omega (nml)
diag(solve(weib.y.nlm$hessian))
```

When $\mathbf{\mathsf{optimHess}}$ is called, parameters are not estimated. In fact, it must be provided as input, being necessary to compute the estimators previously. Since the estimates computed previously refers to $\hat{\omega}$, reparametrization must be used again in the input of the function. The output is given in terms of $\hat{\theta}$ as required.

```{r}
#variances from information matrix of theta (optimHess)
H <- optimHess(theta(weib.y.nlm$estimate),log_lik_weibull,data=y)
diag(solve(H))
```

This result can be compared with the analytical formula (using the $\mathbf{\mathsf{nml}}$ estimate of $\hat{\theta}$).

```{r}
jhat<-matrix(NA,nrow=2,ncol=2)
gammahat <- theta(weib.y.nlm$estimate)[1]
betahat <- theta(weib.y.nlm$estimate)[2]
jhat[1,1]<-n/gammahat^2+sum((y/betahat)^gammahat*
                               (log(y/betahat))^2)
jhat[1,2]<-jhat[2,1]<- n/betahat-sum(y^gammahat/betahat^(gammahat+1)*
                                        (gammahat*log(y/betahat)+1))
jhat[2,2]<- -n*gammahat/betahat^2+gammahat*(gammahat+1)/
   betahat^(gammahat+2)*sum(y^gammahat)
diag(solve(jhat))
```

This variances are almost identical to the previous ones, suggesting a certain precission in numerical estimation of the hessian.

## Exercise 2

### Text:

The Wald confidence interval with level $1−\alpha$ is defined as:
$$ \hat{\gamma} \pm z_{1-\alpha/2}j_{P}(\hat{\gamma})^{-1/2}.$$

Compute the Wald confidence interval of level 0.95 and plot the results.

### Solution:

In order to have the profile likelihood, let's consider the vector $y$ and the log-likelihood of the Weibull Model $l(\gamma, \beta)$:
```{r echo=TRUE,  message=FALSE, warning=FALSE}

y <- c(155.9, 200.2, 143.8, 150.1,152.1, 142.2, 147, 146, 146,
       170.3, 148, 140, 118, 144, 97)
n <- length(y)

log_lik_weibull <- function( data, param){
  -sum(dweibull(data, shape = param[1], scale = param[2], log = TRUE))
}
```


Given this function, the profile likelihood $l_P(\gamma)$ is built evaluating the log-likelihood to $y, \gamma$ and  $\hat{\beta}_{\gamma}= (\sum_{i=1}^{n} y^{\gamma}_{i}/n)^{1/\gamma}$:

```{r echo=TRUE,  message=FALSE, warning=FALSE}
log_lik_weibull_profile_gamma  <- function(data, gamma){
  beta.gamma <- mean(data^gamma)^(1/gamma)
  log_lik_weibull( data, c(gamma, beta.gamma) )
}
```

After having defined the above function, we find the MLE $\hat{\gamma}$ using the $\mathsf{optim()}$ numerical method specifying `hessian=T`. Then, the Wald interval will be defined as $\hat{\gamma} \pm z_{1-\alpha/2}j_{P}(\hat{\gamma})^{-1/2}$, where $j_{P}(\hat{\gamma})$ is the hessian evaluated at $\gamma = \hat{\gamma}$:
```{r echo=TRUE,  message=FALSE, warning=FALSE}
weib_mle<-optim(1 ,fn=log_lik_weibull_profile_gamma,hessian=T,
                  method='L-BFGS-B',lower=rep(1e-7,2),
                  upper=rep(Inf,2),data=y)
weib_mle$par

weib_mle_se<-(weib_mle$hessian[1,1])^(-1/2)
conf_level <- 0.95
weib_mle_ci <- weib_mle$par + c(-1,1)* weib_mle_se * qnorm(1-(1-conf_level)/2)
weib_mle_ci
```

```{r echo=TRUE,  message=FALSE, warning=FALSE}
log_lik_weibull_profile_gamma_v <-Vectorize(log_lik_weibull_profile_gamma, 'gamma'  )
plot(function(x) -log_lik_weibull_profile_gamma_v(data=y, x)+weib_mle$value,
     from=0.1,to=15,xlab=expression(gamma),
     ylab='profile relative log likelihood',
     ylim=c(-8,0))


segments(x0=weib_mle_ci[1], y0=-log_lik_weibull_profile_gamma_v(y,weib_mle_ci[1])+weib_mle$value,
         x1= weib_mle_ci[1], y1=-10,  col="green", lty=2)

segments(x0=weib_mle_ci[2], y0=-log_lik_weibull_profile_gamma_v(y,weib_mle_ci[2])+weib_mle$value, x1= weib_mle_ci[2], y1=-10,  col="green", lty=2)

abline(v=weib_mle$par[1], col="green", lwd=1, lty=1)
text(10, -1, "MLE for Gamma", col="green")
text(7,-7,"95% Wald CI",col="green")
segments( weib_mle_ci[1], -6.7, weib_mle_ci[2], -6.7, col="green", lty =1, lwd=2  )
```

## Exercise 3

### Text:

Repeat the steps above —write the profile log-likelihood, plot it and find the deviance confidence intervals— considering this time $\gamma$ as a nuisance parameter and $\beta$ as the parameter of interest.

### Solution:

In order to evaluate the profile log-likelihood for $\gamma$ as a nuisance parameter and $\beta$ as the parameter of interest we noticed that $\gamma$ is note explicitly expressed in terms of $\beta$. We used the numerical method $\mathsf{uniroot()}$  in order to fix $\gamma = \hat{\gamma}$, since we know that this estimator satisfies for every $\beta$:

$$ \frac{n}{\hat{\gamma}} - n \log(\beta) + \sum_i(\log(y_i)) - \sum_i\Bigg[\bigg(\frac{y_i}{\beta}\bigg)^{\hat{\gamma}} \log\bigg(\frac{y_i}{\beta}\bigg) \Bigg] = 0 $$
Subsequently we proceed to compute the deviance confidence intervals with level $1-\alpha$ as:

$$\{\beta: \ell_P(\beta) \geq \ell_P(\hat \beta) - \frac{1}{2}\chi^2_{1;1-\alpha}\}$$
```{r echo=TRUE,  message=FALSE, warning=FALSE}
gamma <- seq(0.1, 15, length=100)
beta <- seq(100,200, length=100)

weib_mle<-optim(c(1,1),fn=log_lik_weibull,hessian=T,
                  method='L-BFGS-B',lower=rep(1e-7,2),
                  upper=rep(Inf,2),data=y)

log_lik_weibull_profile_beta <- function(data, beta) {
  gamma.beta <- uniroot(function(x) n/x - n * log(beta) + sum(log(data)) - sum((data/beta)^x * log(data/beta)), c(1e-5,15))$root
  log_lik_weibull(data, c(gamma.beta, beta))
}
```


```{r echo=TRUE,  message=FALSE, warning=FALSE}
log_lik_weibull_profile_beta_vec <-Vectorize(log_lik_weibull_profile_beta, 'beta')

plot(function(x) -log_lik_weibull_profile_beta_vec(data=y, x) + weib_mle$value, from=120,to=200,
     xlab=expression(beta),
     ylab='profile relative log likelihood',
     ylim=c(-10,0))

conf.level<-0.95
lrt.ci1<-uniroot(function(x) -log_lik_weibull_profile_beta(data = y, x)+
                   weib_mle$value+
                   qchisq(conf.level,1)/2,
                 c(1e-7,weib_mle$par[2]))$root


lrt.ci1<-c(lrt.ci1,uniroot(function(x) -log_lik_weibull_profile_beta(y,x)+
                             weib_mle$value+
                             qchisq(conf.level,1)/2,
                           c(weib_mle$par[2],200))$root)

abline(h=-qchisq(conf.level,1)/2,lty='dashed',col=2)
segments( lrt.ci1[1],-qchisq(conf.level,1)/2, lrt.ci1[1], 
          -log_lik_weibull_profile_beta_vec(y, lrt.ci1[1]), col="red", lty=2  )
segments( lrt.ci1[2],-qchisq(conf.level,1)/2, lrt.ci1[2],
          -log_lik_weibull_profile_beta_vec(y, lrt.ci1[2]), col="red", lty=2  )
points(lrt.ci1[1], -qchisq(0.95,1)/2, pch=16, col=2, cex=1.5)
points(lrt.ci1[2], -qchisq(0.95,1)/2, pch=16, col=2, cex=1.5)
segments( lrt.ci1[1],
          -8.1, lrt.ci1[2],
          -8.1, col="red", lty =1, lwd=2  )
text(155,-7.5,"95% Deviance CI",col=2)
```

## Exercise 5

### Text:

In **sim** in the code avove you find the MCMC output which allows to approximate the posterior distribution of our parameter of interest with $S$ draws of $\theta$. Please, produce an histogram for these random draws $\theta^{(1)},...,\theta^{(S)}$, compute the empirical quantiles, and overlap the true posterior distribution.

### Solution:

In this model, as said in the lab, the analytical form of the posterior distribution is known. In order to compare the true posterior and the simulated posterior, It has been plotted an histogram with the simulated posterior distribution and a curve for the theoretical one. The histogram has been normalized. Additionally, it are shown the 25, 50 and 75 quantiles, both theoretical and simulated.

```{r}
#EXERCISE 5
library(rstan)

# Make Stan go parallel!
options(mc.cores = parallel::detectCores())

# Avoid losing precious time
rstan::rstan_options(auto_write = TRUE)

#true mean
theta_sample <- 2
#likelihood variance
sigma2 <- 2
#sample size
n <- 10
#prior mean
mu <- 7
#prior variance
tau2 <- 2

#generate some data
set.seed(123)
y <- rnorm(n,theta_sample, sqrt(sigma2))

#posterior mean
mu_star <- ((1/tau2)*mu+(n/sigma2)*mean(y))/( (1/tau2)+(n/sigma2))
#posterior standard deviation
sd_star <- sqrt(1/( (1/tau2)+(n/sigma2)))

#launch Stan model
data<- list(N=n, y=y, sigma =sqrt(sigma2), mu = mu, tau = sqrt(tau2))
fit <- rstan::stan(file="./src/normal.stan", data = data, chains = 4, iter=2000);

#extract Stan output
sim <- rstan::extract(fit);
sample_theta <- sim[["theta"]]
emp_q <- quantile(sample_theta, probs = c(0.25, 0.5, 0.75))
th_q <- qnorm(p = c(0.25, 0.5, 0.75), mu_star, sd_star)

#plot
par(mfrow=c(1,1), pty ="m", oma=c(0,0,0,0))
hist(sim[["theta"]], probability = TRUE, ylim=c(0,1.0), 
     col="darkmagenta", border="white",
     xlab=expression(theta), ylab="Probability", main="Theoretical and simulated dist. with quantiles")
curve(dnorm(x, mu_star, sd_star), col="dodgerblue", lwd=2,
      cex.lab=2, add=T) 
abline(v=emp_q, col="maroon", lwd=2, lty=4)
abline(v=th_q, col="dodgerblue", lwd=2, lty=4)
text((emp_q[1]+th_q[1])/2, 1.0, "q25")
text((emp_q[2]+th_q[2])/2, 1.0, "q50")
text((emp_q[3]+th_q[3])/2, 1.0, "q75")
legend(0.7, 1, c("Simulated", "Theoretical"), 
       c("darkmagenta", "dodgerblue"), lwd=c(2,2), lty=c(4,4), cex=0.8)
```

As can be grafically observed the distributions are quite similar. Also the few quantiles shown in this plot. In a bit deeper diagnostic test, a qq-plot can be performed in order to verify the global behavior of the quantiles.
```{r}
#qqplot

n <- 100
p <- (1:n)/n

q_theor <- qnorm(p = p, mu_star, sd_star)
q_empir <- quantile(sample_theta, probs = p)

par(mfrow=c(1,1), pty ="m", oma=c(0,0,0,0))
plot(x=q_theor,
     y=q_empir,
     type="p",
     col="black", 
     main = "Normal q-q plot",
     xlab="Theoretical quantiles",
     ylab="Empirical quantiles")
segments(x0=0, y0=0, x1=5, y1=5, col="red") 
```

The fit to the theoretical line seems quite good. Also in the extremes where, usually, models tend to fail because of the absence of data.

## Exercise 6

### Text:

Launch the following line of $\mathsf{R}$ code:

```
posterior <- as.array(fit)
```

Use now the $\mathsf{bayesplot}$ package. Read the help and produce for this example, using the object posterior, the following plots:

- posterior intervals;
- posterior areas;
- marginal posterior distributions for the parameters.

Quickly comment.


### Solution:

```{r lab_06, code = readLines("src/lab_06.R"), echo=TRUE}
```

### Comment:

XYZ

## Exercise 7

### Text:

Suppose you receive $n=15$ phone calls in a day, and you want to build a model to assess their average length. Your likelihood for each call length is $y_i \sim \mathrm{Poisson}(\lambda)$. Now, you have to choose the prior $\pi(\lambda)$. Please, tell which of these priors is adequate to describe the problem, and provide a short motivation for each of them:

1. $\pi(\lambda)=\mathrm{Beta}(4, 2)$;
2. $\pi(\lambda)=\mathrm{Normal}(1, 2)$;
3. $\pi(\lambda)=\mathrm{Gamma}(4, 2)$;

Now, compute your posterior as $\pi(\lambda|y) \propto L(\lambda;y)\pi(\lambda)$  for the selected prior. If your first choice was correct, you will be able to compute it analitically.

### Solution:

First of all, let's analyse the different proposed distributions. Let's use a plot to see in a clear way the main features of these distributions.

```{r}
par(mfrow=c(2,2), pty ="m", oma=c(0,0,0,0))
curve(dbeta(x, 4, 2), col="dodgerblue", lwd=2, main="Beta distribution") # x compresa fra 0 e 1 BAD
curve(dnorm(x, 1, 2), col="dodgerblue", lwd=2, xlim=c(-3,5), main="Normal distribution") # x con valori negativi BAD
curve(dgamma(x, 4, 2), col="dodgerblue", lwd=2, xlim=c(0,5), main="Gamma distribution") # x con valori positivi e non necessariamente ristretti GOOD

```

It is important to note that the model must simulate lengths of calls, which is a measurement of time (a real value always greater than 0). The first proposed model is a $\mathrm{Poisson}(\lambda)$ distribution in which $\lambda$ is both the mean and the variance of the distribution, in this case, the mean on the length of the calls. The first proposed distribution is the $\mathrm{Beta}(4, 2)$, corresponding to the first plot. This distribution confines the $\lambda$ between 0 and 1. It is not a realistic behavior as a phone call can have a length greater than 1, so it is not much reliable as prior assumption. The second proposed prior is a normal. A normal is able to return negative values of $\lambda$ that, actually, does not make sense in the context of a time measurement. The only suitable distribution as prior of $\lambda$ is the Gamma one. A gamma returns positive (non zero) samples of $\lambda$, which corresponds exacly with its natural domain.

Posterior can be computed with $\mathsf{stan}$. The first step is to program the model:

```{stan output.var = 'foo_jose'}
data{
  int N;
  int y[N];
  real<lower=0> alpha;
  real<lower=0> beta;
}
parameters{
  real<lower=0> lambda;
}
model{
  target += poisson_lpmf( y | lambda );
  target += gamma_lpdf(lambda|alpha, beta);
}
```
with the model ready, posterior can be computed. In order to compute the model, 15 samples have been generated by using a $\mathrm{Poisson}(\lambda=4)$.

```{r}
#simulate the data
set.seed(123)
n <- 15
true_mean <- 4
y <- rpois(n,true_mean)

#prior params
alpha<-4
beta<-2

#launch stan model
data<- list(N=n, y=y, alpha=alpha, beta=beta)
fit2 <- rstan::stan(file="./src/gamma_lambda.stan", data = data, chains = 4, iter=2000, refresh=-1);

#extract Stan output
sim <- rstan::extract(fit2)
sample_lambda <- sim[["lambda"]]

#posterior params
alpha_star = alpha + length(y)*mean(y)
beta_star = beta+length(y)

#plot
par(mfrow=c(1,1), pty ="m", oma=c(0,0,0,0))
hist(sim[["lambda"]], probability = TRUE, xlim=c(0,7), ylim=c(0,1.0), breaks=15, 
     col="darkmagenta", border="white",
     xlab=expression(lambda), ylab="Probability", main="Prior and posterior distribution")
curve(dgamma(x, 4, 2), col="dodgerblue", lwd=2,
      cex.lab=2, add=T) 
curve(dgamma(x, alpha_star, beta_star), col="maroon", lwd=2,
      cex.lab=2, add=T)
abline(v=true_mean, col="darkgoldenrod1", lwd=3, lty=1)
legend(0, 1, c("Posterior", "Th. Posterior", "Prior", expression(paste("True ", lambda))), 
       c("darkmagenta", "maroon", "dodgerblue", "darkgoldenrod1"), cex=0.8)
```

In the posterior distribution it is observed a maximum near to the true value of $\lambda$. The particular prior distribution and likelihood used in this simulation permit to compute also analitically the theoretical posterior distribution. As will be shown, the prior is the conjugate of the posterior. In the computation, multiplicative constants will be avoided in order to clarify the explanation. The first step is to write the likelihood and the prior a more friendly way:

$$
p(\mathbf{y}|\lambda)  =  \prod_{i=1}^N p(y_i|\lambda)  
              =  \prod_{i=1}^N \frac{e^{-\lambda} \lambda^{y_i}}{y_i!} 
              \propto e^{-N\lambda}\lambda^{N \bar{y}}; \ \ \ \
p(\lambda)  =  \frac{\beta^\alpha \lambda^{\alpha-1}  e^{-\beta \lambda}}{\Gamma(\alpha)}
            \propto  \lambda^{\alpha-1} e^{-\beta \lambda}
$$

Now both (simplified) expressions are multiplied.


$$
p(\lambda | \mathbf{y}) \ = \ p(\mathbf{y} | \lambda)p(\lambda)
                        \ \propto \ \lambda^{\alpha-1} e^{-\beta \lambda} e^{-N \lambda} \lambda^{N\bar{y}}
                         =  \lambda^{\alpha-1+N \bar{y}} e^{-\lambda (\beta + N)}
$$
In this formula, constants can be renamed as follows:
$$
\begin{align}
\alpha' &= \alpha + N \bar{y} \\
\beta' &= \beta + N
\end{align}
$$
The resulting formula then is the next:
$$
p(\lambda | \mathbf{y}) \ \propto \ \lambda^{\alpha'-1} e^{-\lambda \beta'} 
$$
This expression corresponds to the functional form of a $\mathrm{Gamma}(\alpha', \beta')$ distribution except for a multiplicative constant, which can be also seen as a normalization constant. Since the quantity $p(\lambda | \mathbf{y})$ must be necessarily a probability distribution, the only possible choice for the unknown normalization constant is to be the one of the Gamma distribution, so the complete posterior distribution is the next:

$$
p(\lambda | \mathbf{y}) = \mathrm{Gamma}(\alpha', \beta'); \ \ \ \ 
\alpha' = \alpha + N \bar{y}, \ \ \beta' = \beta + N
$$

Since it is known the analytical expression of the posterior, is easy to check the goodness of the Markov samples with a qqplot:

```{r}
#qqplot
n <- 100
p <- (1:n)/n

q_theor <- qgamma(p = p, alpha_star, beta_star)
q_empir <- quantile(sample_lambda, probs = p)

par(mfrow=c(1,1), pty ="m", oma=c(0,0,0,0))
plot(x=q_theor,
     y=q_empir,
     type="p",
     col="black", 
     main = "Gamma q-q plot",
     xlab="Theoretical quantiles",
     ylab="Empirical quantiles")
segments(x0=0, y0=0, x1=5.5, y1=5.5, col="red") 

```

The Markov process samples fits well in the qqplot, in a similar way as samples of the exercise 5.

## Exercise 8

### Text:

Go to this link: [rstan](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started), and follow the instructions to download and install the rstan library. Once you did it succesfully, open the file model called biparametric.stan, and replace the line

$$ \mathsf{ target+=cauchy\_lpdf(sigma| 0, 2.5);} $$ 
**with the following one:**

$$ \mathsf{ target+=uniform_lpdf(sigma|0.1,10);} $$
Which prior are you now assuming for your parameter $\sigma$? Reproduce the same plots as above and briefly comment.

### Solution:

The `biparametric.stan` file used has the following structure:
```{stan output.var = 'foo_rob'}
data{
  int N;
  real y[N];
  real a;
  real b;
}
parameters{
  real theta;
  real<lower=0> sigma;
}
model{
  target+=normal_lpdf(y|theta, sigma);
  target+=uniform_lpdf(theta|a, b );
  target+=uniform_lpdf(sigma|0.1,10);
}
```
In this case we're assuming that the parameter $\sigma$ has a prior distribution that is Uniform in the interval $[0.1,10]$
```{r echo=TRUE,  message=FALSE, warning=FALSE}
#true mean
theta_sample <- 2
#likelihood variance
sigma2 <- 2
#sample size
n <- 10

#generate some data
set.seed(123)
y <- rnorm(n,theta_sample, sqrt(sigma2))
```

```{r echo=TRUE,  message=FALSE, warning=FALSE}
library(rstan)
library(bayesplot)
library(rstanarm)
library(ggplot2)

# Make Stan go parallel!
options(mc.cores = parallel::detectCores())

# Avoid losing precious time
rstan::rstan_options(auto_write = TRUE)


data3<- list(N=n, y=y, a=-10, b=10)
fit3 <- rstan::stan(file="./src/biparametric.stan", data = data3, chains = 4, iter=2000,
             refresh=-1)
#extract stan output for biparametric model
sim3 <- rstan::extract(fit3)
posterior_biv <- as.matrix(fit3)
theta_est <- mean(sim3$theta)
sigma_est <- mean(sim3$sigma)
#mean of the samples
c(theta_est, sigma_est)
```

Once we have the MCMC samples, we represent the result with `mcmc_areas`: 
```{r echo=TRUE,  message=FALSE, warning=FALSE}
plot_title <- ggplot2::ggtitle("Posterior distributions", "with medians and 80% intervals")
bayesplot::mcmc_areas(posterior_biv, pars = c("theta","sigma"), prob = 0.8) + plot_title
```

From the result of the MCMC sampling we observe that mean and variance results for both $\theta$ and $\sigma$ are slightly different from the case in wich $\sigma \sim \text{Cauchy}^+ (0,2.5)$. This result suggests that priors used don't provide much information to the posterior distributions.

## Exercise 9

### Text:

Reproduce the first plot above for the soccer goals, but this time by replacing Prior 1 with a $\mathsf{Gamma}(2,4)$. Then, compute the final Bayes factor matrix ($\mathsf{BF\_matrix}$) with this new prior and the other ones unchanged, and comment. Is still Prior 2 favorable over all the others?

### Solution:

In this exercise we are interested in assessing the average number of goals scored by a given team in Major league Soccerr, and denote the goals for $n$ games as $y_1,…,y_n$.
```{r echo=TRUE,  message=FALSE, warning=FALSE}
library(LearnBayes)
data(soccergoals)

y <- soccergoals$goals
```
So we assume a poisson likelihood. Then we have to elicit a prior for the parameter $\mathsf{\lambda}$, with several plausible choices.\
We'll use the same code provided by the lab exercitation, but this time we'll change the first prior to compare in a $\mathsf{Gamma}(2,4)$ distribution.
```{r echo=TRUE,  message=FALSE, warning=FALSE}
#write the likelihood function via the gamma distribution

lik_pois<- function(data, theta){
  n <- length(data)
  lambda <- exp(theta)
  dgamma(lambda, shape =sum(data)+1, scale=1/n)
}

prior_gamma <- function(par, theta){
  lambda <- exp(theta)
  dgamma(lambda, par[1], rate=par[2])*lambda  
}

prior_norm <- function(npar, theta){
  lambda=exp(theta)  
  (dnorm(theta, npar[1], npar[2]))
  
}

lik_pois_v <- Vectorize(lik_pois, "theta")
prior_gamma_v <- Vectorize(prior_gamma, "theta")
prior_norm_v <- Vectorize(prior_norm, "theta")


#likelihood
curve(lik_pois_v(theta=x, data=y), xlim=c(-3,4), xlab=expression(theta), ylab = "density", lwd =2 )
#prior 1
curve(prior_gamma_v(theta=x, par=(c(2, 4))), lty =2, col="red", add = TRUE, lwd =2)
#prior 2 
curve(prior_norm_v(theta=x, npar=c(1, 0.5)), lty =3, col="blue", add =TRUE, lwd=2)
#prior 3 
curve(prior_norm_v(theta=x, npar=c(2, 0.5)), lty =4, col="green", add =TRUE, lwd =2)
#prior 4 
curve(prior_norm_v(theta=x, npar=c(1, 2)), lty =5, col="violet", add =TRUE, lwd =2)
legend(2.6, 1.8, c("Lik.", "Ga(4.57,1.43)", "N(1, 0.25)", "N(2,0.25)","N(1, 4)" ),
       lty=c(1,2,3,4,5), col=c("black", "red", "blue", "green", "violet"),lwd=2, cex=0.9)
```
As we can already see from the plot, the new prior we implemented (red) is 

Now we compute and display BF matrix:
```{r echo=TRUE,  message=FALSE, warning=FALSE}
logpoissongamma <- function(theta, datapar){
  data <- datapar$data
  par <- datapar$par
  lambda <- exp(theta)
  log_lik <- log(lik_pois(data, theta))
  log_prior <- log(prior_gamma(par, theta))
  return(log_lik+log_prior)
}

logpoissonnormal <- function( theta, datapar){
  data <- datapar$data
  npar <- datapar$par
  lambda <- exp(theta)
  log_lik <- log(lik_pois(data, theta))
  log_prior <- log(prior_norm(npar, theta))
  return(log_lik+log_prior)
}  

datapar <- list(data=y, par=c(2, 4))
fit1 <- LearnBayes::laplace(logpoissongamma, .5, datapar)
datapar <- list(data=y, par=c(1, .5))
fit2 <- LearnBayes::laplace(logpoissonnormal, .5, datapar)
datapar <- list(data=y, par=c(2, .5))
fit3 <- LearnBayes::laplace(logpoissonnormal, .5, datapar)
datapar <- list(data=y, par=c(1, 2))
fit4 <- LearnBayes::laplace(logpoissonnormal, .5, datapar)

logmarg <- c(fit1$int, fit2$int, fit3$int, fit4$int)

BF_matrix <- matrix(1, 4,4)
for (i in 1:3){
  for (j in 2:4){
    BF_matrix[i,j]<- exp(logmarg[i]-logmarg[j])
    BF_matrix[j,i]=(1/BF_matrix[i,j]) 
  }
}

round_bf <- round(BF_matrix,3)
round_bf
```

## Exercise 10

### Text:

Let $y=(1,0,0,1,0,0,0,0,0,1,0,0,1,0)$ collect the results of tossing $n=14$ times an unfair coin, where $1$ denotes $heads$ and $0$ $tails$, and $p=Prob(y_i=1)$.

* Looking at the $\mathsf{Stan}$ code for the other models, write a short $\mathsf{Stan}$ Beta-Binomial model, where $p$ has a $\mathsf{Beta}(a,b)$ prior with $a=3$,$b=3$.
* extract the posterior distribution with the function $\mathsf{extract}()$;
* produce some plots with the $\mathsf{bayesplot}$ package and comment.
* compute analitically the posterior distribution and compare it with the $\mathsf{Stan}$ distribution.

### Solution

First of all we load data,
```{r echo=TRUE,  message=FALSE, warning=FALSE}
library(rstan)
library(ggplot2)
library(bayesplot)

y <- c(1,0,0,1,0,0,0,0,0,1,0,0,1,0)
n <- 14
a <- 3
b <- 3
```
the `ex10.stan` file used has the following structure:
```{stan output.var = 'foo_rob'}
data{
  int N;
  int y[N];
  real alpha;
  real beta;
}
parameters{
  real<lower=0,upper=1> p;
}
model{
  target+=bernoulli_lpmf( y | p );
  target+=beta_lpdf( p | alpha, beta );
}
```
at this point we launch the Stan model:
```{r echo=TRUE,  message=FALSE, warning=FALSE}
data<- list(N=n, y=y, alpha=a , beta=b)
fit <- rstan::stan(file="ex10.stan", data = data, chains = 4, iter=2000)
```
we extract the posterior distribution:
```{r echo=TRUE,  message=FALSE, warning=FALSE}
#extract Stan output
sim <- rstan::extract(fit)
```
and with bayesplot we analyse the results of the model.
```{r echo=TRUE,  message=FALSE, warning=FALSE}
#traceplot
bayesplot::traceplot(fit, pars ="p")


#MCMC areas
posterior <- as.matrix(fit)

plot_title <- ggplot2::ggtitle("Posterior distributions",
                      "with medians and 80% intervals")

bayesplot::mcmc_areas(posterior, 
           pars = "p", 
           prob = 0.8) + plot_title
```
In the end we compute analytically the posterior distribution
$$
\mathsf{\pi}(p|y) \propto \mathsf{L}(p;y)\mathsf{\pi}(p) \\
\mathsf{\pi}(p|y) \propto \mathsf{Binom}(y;n,p)\mathsf{Beta}(a,b) \\
\mathsf{\pi}(p|y) \propto p^y(1-p)^{n-y}p^{a-1}(1-p)^{b-1} = p^{y+a-1}(1-p)^{n-y+b-1} \\
\mathsf{\pi}(p|y) \propto \mathsf{Beta}(a+y,b+n-y)
$$
In our specific case we have:
$$
\mathsf{\pi}(p|y) \propto \mathsf{Beta}(7,13)
$$
Finally we plot and compare the true analytical posterior and the Stan simulated posterior
```{r echo=TRUE,  message=FALSE, warning=FALSE}
par(mfrow=c(1,2), pty ="m", oma=c(0,0,0,0))

curve(dbeta(x, 3, 3),xlim=c(-1,2), lty=1, lwd=2, col="red", ylim=c(0,4), 
      ylab="density", xlab=expression(p), cex.lab=2)
lines(density(sim$p, adj=2), col ="blue", lwd=2, lty =1)
legend(0.5, 4, c("Prior", "Stan Posterior"),
       c("red", "blue" ), lty=c(1,1),lwd=c(1,1), cex=0.5)


curve(dbeta(x, 3, 3),xlim=c(-1,2), lty=1, lwd=2, col="red", ylim=c(0,4), 
      ylab="density", xlab=expression(p), cex.lab=2)
curve(dbeta(x, 7, 13), 
      xlab=expression(theta), ylab="", col="blue", lwd=2,
      cex.lab=2, add=T)  
legend(0.5, 4, c("Prior", "Analytical Posterior"), 
       c("red", "blue" ), lty=c(1,1),lwd=c(1,1), cex=0.5)
```
and we also compare them in the same plot
```{r echo=TRUE,  message=FALSE, warning=FALSE}
par(mfrow=c(1,1), pty ="m", oma=c(0,0,0,0))


curve(dbeta(x, 7, 13),xlim=c(-1,2), lty=2, lwd=1, col="black", ylim=c(0,4), 
      ylab="density", xlab=expression(theta), cex.lab=2)
curve(dbeta(x, 3, 3 ), xlim=c(-1,2), col="red", lty=1,lwd=2,  add =T)
lines(density(sim$p, adj=2), col ="blue", lwd=2, lty =1)
legend(0.5, 4, c("Prior", "Analytical posterior", "Stan Posterior"),
       c("red", "black", "blue", "blue" ), lty=c(1,2,1),lwd=c(1,1,2), cex=0.8)
```
We can barely see the difference between the Stan posterior and the analytical one. 

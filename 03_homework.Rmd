---
title: "SMDS Homework - Block 3"
author: "R. Corti, L. Taroni, J.A. Fernandez Santisteban and E. Ballarin  |  Group 'D'"
date: "13th May 2020"
output:
  html_document:
    theme: darkly
    highlight: breezedark
    mathjax: default
    self_contained: true
    md_extensions: +autolink_bare_uris
    toc: true
    toc_collapsed: false
    toc_float: true
    toc_depth: 3
    number_sections: false
header-includes:
- \usepackage{color}
- \usepackage{graphicx}
- \usepackage{grffile}
institute: University of Trieste, SISSA, ICTP, University of Udine
graphics: yes
fontsize: 10pt
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.align = 'center', warning=FALSE, message=FALSE, fig.asp=0.625, dev='png', global.par = TRUE, dev.args=list(pointsize=10), fig.path = 'figs/')
```

```{r setup, include=FALSE}
library(knitr)
local({
  hook_plot = knit_hooks$get('plot')
  knit_hooks$set(plot = function(x, options) {
    paste0('\n\n----\n\n', hook_plot(x, options))
  })
})
```

# Exercises from *LEC*

## Exercise 1

### Text:

Compute the *bootstrap*-based confidence interval for the $\mathsf{score}$ dataset using the *studentized* method.

### Solution:

```{r lec_01, code = readLines("src/lec_01.R"), echo=TRUE}
```

### Comment:

The exercise has been solved by estimating (since required and not explicitly given) *standard errors* for the statistic of interest in each *bootstrap* resample via another *bootstrap* iteration. This schema is known as *bootstrap-in-bootstrap*; similarly *jackknife-in-bootstrap* provides an equally viable (though linearly-approximated) solution to that problem.

Parallelization of independent *bootstrap* iterations is possible via the useage of the $\mathsf{doParallel}$ package, but has not been pursued due to potential compatibility breakage with R 4.0.0 *Arbor Day* (which has been used).

## Exercise 2

### Text:

Compute *bootstrap*-based confidence intervals for the $\mathsf{score}$ dataset using the $\mathsf{boot}$ package.

### Solution:

```{r lec_02, code = readLines("src/lec_02.R"), echo=TRUE}
```

### Comment:

As for the previous exercise, also in this case *bootstrap-in-bootstrap* has been used. All *bootstrap* iterations are implemented by using the $\mathsf{boot}$ package.

Parallelization of independent *bootstrap* iterations is possible via the usage of the package-integrated option `parallel = c("multicore")`, but has not been pursued due to potential compatibility breakage with R 4.0.0 *Arbor Day* (which has been used).


# Exercises from *LAB*

## Exercise 1

### Text:

Use $\mathbf{\mathsf{nml}}$ to compute the variance for the estimator $\hat{\omega}=(\log(\hat{\gamma}),\log(\hat{\beta}))$ and $\mathbf{\mathsf{optimHess}}$ for the variance of $\hat{\theta}=(\hat{\gamma},\hat{\beta})$.

### Solution:

The estimate of the $\hat{\omega}$ using $\mathbf{\mathsf{nml}}$ can be simply made by passing the reparametrization $\hat{\omega}=(\log( \hat{\gamma}), \log(\hat{ \beta}))$ as the input of $\mathbf{\mathsf{log\_lik\_weibull}}$. In such way, $\mathbf{\mathsf{nml}}$ does not optimize w.r.t. the parameter $\hat{\theta}$, but w.r.t. $\hat{\omega}$ directly. The Hessian matrix, too, is being computed in terms of the reparametrized $\hat{\omega}$. At this point, the procedure consists in just the computation of the hessian (via builtin `hessian=T`), and the computation of its inverse matrix, which contains the variance of $\hat{\omega}$.

```{r}
y <- c(155.9, 200.2, 143.8, 150.1,152.1, 142.2, 147, 146, 146, 170.3, 148, 140, 118, 144, 97)
n <- length(y)

# log-likelihood function
log_lik_weibull <- function( data, param){
   -sum(dweibull(data, shape = param[1], scale = param[2], log = TRUE))
}

theta <- function(omega) exp(omega)

log_lik_weibull_rep <- function(data, param) log_lik_weibull(data, theta(param))

weib.y.nlm<-nlm(log_lik_weibull_rep,c(0,0),hessian=T,data=y)
weib.y.nlm

# variances from information matrix of omega (nml)
diag(solve(weib.y.nlm$hessian))
```

For what concerns the computation of the variances of $\hat{\theta}=(\hat{\gamma},\hat{\beta})$ via $\mathbf{\mathsf{optimHess}}$, parameters are not estimated upon call. In fact, they must be provided as additional input, since necessary to compute the estimator. Willing to re-use previously-computed estimates, which refer to $\hat{\omega}$, inverse reparametrization must be computed and applied. The output is given in terms of $\hat{\theta}$ as required.

```{r}
# variances from information matrix of theta (optimHess)
H <- optimHess(theta(weib.y.nlm$estimate),log_lik_weibull,data=y)

diag(solve(H))
```

This result can be compared with the analytical formula (using the $\mathbf{\mathsf{nml}}$ estimate of $\hat{\theta}$).

```{r}
jhat<-matrix(NA,nrow=2,ncol=2)

gammahat <- theta(weib.y.nlm$estimate)[1]
betahat <- theta(weib.y.nlm$estimate)[2]

jhat[1,1]<-n/gammahat^2+sum((y/betahat)^gammahat*(log(y/betahat))^2)
jhat[1,2]<-jhat[2,1]<- n/betahat-sum(y^gammahat/betahat^(gammahat+1)*(gammahat*log(y/betahat)+1))
jhat[2,2]<- -n*gammahat/betahat^2+gammahat*(gammahat+1)/betahat^(gammahat+2)*sum(y^gammahat)

diag(solve(jhat))
```

Such just-computed variances are almost identical to the previous ones, suggesting a certain precision in numerical estimation of the Hessian.


## Exercise 2

### Text:

The Wald confidence interval with level $1−\alpha$ is defined as:
$$ \hat{\gamma} \pm z_{(1-\alpha/2)}\ j_{P}(\hat{\gamma})^{-1/2}.$$

Compute the Wald confidence interval of level $0.95$ and plot the results (for the Weibull model just outlined before).

### Solution:

In order to obtain the profile likelihood, let us consider the vector $y$ and the log-likelihood of the Weibull Model $l(\gamma, \beta)$:
```{r echo=TRUE,  message=FALSE, warning=FALSE}

y <- c(155.9, 200.2, 143.8, 150.1,152.1, 142.2, 147.0, 146.0, 146.0, 170.3, 148.0, 140.0, 118.0, 144.0, 97.0)
n <- length(y)

log_lik_weibull <- function(data, param){
  -sum(dweibull(data, shape = param[1], scale = param[2], log = TRUE))
}
```


Given such log-likelihood, the profile (log-)likelihood $l_P(\gamma)$ is built evaluating the log-likelihood at $y, \gamma$ and $\hat{\beta}_{\gamma}= (\sum_{i=1}^{n} y^{\gamma}_{i}/n)^{1/\gamma}$:

```{r echo=TRUE,  message=FALSE, warning=FALSE}
log_lik_weibull_profile_gamma  <- function(data, gamma){
  beta.gamma <- mean(data^gamma)^(1/gamma)
  log_lik_weibull( data, c(gamma, beta.gamma))
}
```

Now we find the ML estimate $\hat{\gamma}$ using the $\mathsf{optim()}$ numerical optimization method, obtaining the Hessian as a byproduct, by specifying `hessian=T` among its arguments. Then, the Wald interval is defined as $$ \hat{\gamma} \pm z_{(1-\alpha/2)}\ j_{P}(\hat{\gamma})^{-1/2}.$$, where $j_{P}(\hat{\gamma})$ is the hessian evaluated at $\gamma = \hat{\gamma}$:
```{r echo=TRUE,  message=FALSE, warning=FALSE}
weib_mle<-optim(1, fn=log_lik_weibull_profile_gamma,hessian=T,
                   method='L-BFGS-B',lower=rep(1e-7,2),
                   upper=rep(Inf,2),data=y)

weib_mle$par

weib_mle_se<-(weib_mle$hessian[1,1])^(-1/2)

conf_level <- 0.95

weib_mle_ci <- weib_mle$par + c(-1,1) * weib_mle_se * qnorm(1-(1-conf_level)/2)
weib_mle_ci
```

```{r echo=TRUE,  message=FALSE, warning=FALSE}
log_lik_weibull_profile_gamma_v <-Vectorize(log_lik_weibull_profile_gamma, 'gamma')

plot(function(x) -log_lik_weibull_profile_gamma_v(data=y, x)+weib_mle$value,
     from=0.1,to=15,xlab=expression(gamma),
     ylab='profile relative log likelihood',
     ylim=c(-8,0))


segments(x0=weib_mle_ci[1], y0=-log_lik_weibull_profile_gamma_v(y,weib_mle_ci[1])+weib_mle$value,
         x1= weib_mle_ci[1], y1=-10,  col="green", lty=2)

segments(x0=weib_mle_ci[2], y0=-log_lik_weibull_profile_gamma_v(y,weib_mle_ci[2])+weib_mle$value, x1= weib_mle_ci[2], y1=-10,  col="green", lty=2)

abline(v=weib_mle$par[1], col="green", lwd=1, lty=1)

text(10, -1, "MLE for Gamma", col="green")
text(7,-7,"95% Wald CI",col="green")

segments( weib_mle_ci[1], -6.7, weib_mle_ci[2], -6.7, col="green", lty =1, lwd=2  )
```

## Exercise 3

### Text:

Repeat the steps above -- write the profile log-likelihood, plot it and find the deviance confidence intervals -- considering this time $\gamma$ as a nuisance parameter and $\beta$ as the parameter of interest.

### Solution:

In order to evaluate the profile log-likelihood for $\gamma$ as a nuisance parameter and $\beta$ as the parameter of interest we notice that $\gamma$ is not explicitly expressed in terms of $\beta$. We have to use, then, the numerical method $\mathsf{uniroot()}$ in order to obtain the value which should be fixed $\gamma = \hat{\gamma}$, since we know that this estimator satisfies for every $\beta$ the following:

$$ \frac{n}{\hat{\gamma}} - n \log(\beta) + \sum_i(\log(y_i)) - \sum_i\Bigg[\bigg(\frac{y_i}{\beta}\bigg)^{\hat{\gamma}} \log\bigg(\frac{y_i}{\beta}\bigg) \Bigg] = 0 $$

Subsequently we proceed to compute the deviance confidence intervals with level $1-\alpha$ as:

$$\{\beta: \ell_P(\beta) \geq \ell_P(\hat \beta) - \frac{1}{2}\chi^2_{1;1-\alpha}\}$$

```{r echo=TRUE,  message=FALSE, warning=FALSE}
gamma <- seq(0.1, 15, length=100)
beta <- seq(100,200, length=100)

weib_mle<-optim(c(1,1),fn=log_lik_weibull,hessian=T,
                  method='L-BFGS-B',lower=rep(1e-7,2),
                  upper=rep(Inf,2),data=y)

log_lik_weibull_profile_beta <- function(data, beta) {
  gamma.beta <- uniroot(function(x) n/x - n * log(beta) + sum(log(data)) - sum((data/beta)^x * log(data/beta)), c(1e-5,15))$root
  log_lik_weibull(data, c(gamma.beta, beta))
}
```


```{r echo=TRUE,  message=FALSE, warning=FALSE}
log_lik_weibull_profile_beta_vec <- Vectorize(log_lik_weibull_profile_beta, 'beta')

plot(function(x) -log_lik_weibull_profile_beta_vec(data=y, x) + weib_mle$value, from=120, to=200,
     xlab = expression(beta),
     ylab = 'profile relative log likelihood',
     ylim = c(-10,0))

conf.level<-0.95

lrt.ci1 <- uniroot(function(x) -log_lik_weibull_profile_beta(data = y, x) +
                   weib_mle$value +
                   qchisq(conf.level,1)/2,
                   c(1e-7,weib_mle$par[2]))$root


lrt.ci1<-c(lrt.ci1, uniroot(function(x) -log_lik_weibull_profile_beta(y,x) +
                             weib_mle$value +
                             qchisq(conf.level,1)/2,
                             c(weib_mle$par[2],200))$root)

abline(h = -qchisq(conf.level,1)/2, lty='dashed', col=2)

segments(lrt.ci1[1], -qchisq(conf.level,1)/2, lrt.ci1[1], 
          -log_lik_weibull_profile_beta_vec(y, lrt.ci1[1]),
          col="red", lty=2)

segments( lrt.ci1[2],-qchisq(conf.level,1)/2, lrt.ci1[2],
          -log_lik_weibull_profile_beta_vec(y, lrt.ci1[2]), col="red", lty=2)

points(lrt.ci1[1], -qchisq(0.95,1)/2, pch=16, col=2, cex=1.5)

points(lrt.ci1[2], -qchisq(0.95,1)/2, pch=16, col=2, cex=1.5)
segments( lrt.ci1[1],
          -8.1, lrt.ci1[2],
          -8.1, col="red", lty =1, lwd=2)

text(155,-7.5,"95% Deviance CI",col=2)
```

## Exercise 5

### Text:

The $\mathsf{sim}$ variable in the code above contains the MCMC output that approximates the posterior distribution of our parameter of interest, obtained from with $S$ draws of $\theta$. Please, produce an histogram for these random draws $\theta^{(1)},...,\theta^{(S)}$, compute the empirical quantiles, and overlap to it the true posterior distribution.

### Solution:

For this model the analytical, closed form, of the posterior distribution is known. In order to compare it to the simulated (approximate) posterior, the following plot has been produced. It shows an histogram with the simulated posterior distribution and the curve of the theoretical one. The histogram has been normalized. Additionally, the 0.25, 0.50 and 0.75 quantiles are shown -- both theoretical and computed from the simulation.

```{r}
library(rstan)

# Make Stan go parallel!
options(mc.cores = parallel::detectCores())

# Avoid losing precious time
rstan::rstan_options(auto_write = TRUE)

# true mean
theta_sample <- 2

# likelihood variance
sigma2 <- 2

# sample size
n <- 10

# prior mean
mu <- 7

# prior variance
tau2 <- 2

# generate some data!
set.seed(123)
y <- rnorm(n,theta_sample, sqrt(sigma2))

# posterior mean
mu_star <- ((1/tau2)*mu+(n/sigma2)*mean(y))/((1/tau2)+(n/sigma2))

# posterior standard deviation
sd_star <- sqrt(1/((1/tau2)+(n/sigma2)))

# Fit Stan model
data <- list(N=n, y=y, sigma =sqrt(sigma2), mu = mu, tau = sqrt(tau2))
fit <- rstan::stan(file="./src/normal.stan", data = data, chains = 4, iter=2000);

# extract Stan output
sim <- rstan::extract(fit);
sample_theta <- sim[["theta"]]

emp_q <- quantile(sample_theta, probs = c(0.25, 0.5, 0.75))
th_q <- qnorm(p = c(0.25, 0.5, 0.75), mu_star, sd_star)

# plot
par(mfrow=c(1,1), pty ="m", oma=c(0,0,0,0))

hist(sim[["theta"]], probability = TRUE, ylim=c(0,1.0), 
     col="darkmagenta", border="white",
     xlab=expression(theta), ylab="Probability", main="Theoretical and simulated dist. with quantiles")

curve(dnorm(x, mu_star, sd_star), col="dodgerblue", lwd=2,
      cex.lab=2, add=T)

abline(v=emp_q, col="maroon", lwd=2, lty=4)
abline(v=th_q, col="dodgerblue", lwd=2, lty=4)

text((emp_q[1]+th_q[1])/2, 1.0, "q25")
text((emp_q[2]+th_q[2])/2, 1.0, "q50")
text((emp_q[3]+th_q[3])/2, 1.0, "q75")

legend(0.7, 1, c("Simulated", "Theoretical"), 
       c("darkmagenta", "dodgerblue"), lwd=c(2,2), lty=c(4,4), cex=0.8)
```

As it can be grafically observed, the distributions are quite similar. This is also true for the few quantiles shown in the plot. For a deeper diagnostic test, a *quantile-quantile plot* can be obtained in order to compare the global behavior of the quantiles.

```{r}
n <- 100
p <- (1:n)/n

q_theor <- qnorm(p = p, mu_star, sd_star)
q_empir <- quantile(sample_theta, probs = p)

par(mfrow=c(1,1), pty ="m", oma=c(0,0,0,0))
plot(x=q_theor,
     y=q_empir,
     type="p",
     col="black", 
     main = "Normal q-q plot",
     xlab="Theoretical quantiles",
     ylab="Empirical quantiles")

segments(x0=0, y0=0, x1=5, y1=5, col="red") 
```

The comparison to the theoretical line seems good; included at the extremes where, usually, models tend to fail because of the absence of data.


## Exercise 6

### Text:

Launch the following line of $\mathsf{R}$ code (starting from the already-defined codebase):

```
posterior <- as.array(fit)
```

Use now the $\mathsf{bayesplot}$ package. Read the help and produce for this example, using the object posterior, the following plots:

- posterior intervals;
- posterior areas;
- marginal posterior distributions for the parameters.

Quickly comment.


### Solution:

```{r lab_06, code = readLines("src/lab_06.R"), echo=TRUE}
```

### Comment:

#FIXME!

## Exercise 7

### Text:

Suppose you receive $n=15$ phone calls in a day, and you want to build a model to assess their average length. Your likelihood for each call length is $y_i \sim \mathrm{Poisson}(\lambda)$. Now, you have to choose the prior $\pi(\lambda)$. Please, tell which of these priors is adequate to describe the problem, and provide a short motivation for each of them:

1. $\pi(\lambda)=\mathrm{Beta}(4, 2)$;
2. $\pi(\lambda)=\mathrm{Normal}(1, 2)$;
3. $\pi(\lambda)=\mathrm{Gamma}(4, 2)$;

Now, compute your posterior as $\pi(\lambda|y) \propto L(\lambda;y)\pi(\lambda)$  for the selected prior. If your first choice was correct, you will be able to compute it analitically.

### Solution:

First of all, let us analyse the different proposed distributions. Let us use a plot to see in a clear way the main features for each of them.

```{r}
par(mfrow=c(2,2), pty ="m", oma=c(0,0,0,0))
curve(dbeta(x, 4, 2), col="dodgerblue", lwd=2, main="Beta distribution") # [0,1): not good.
curve(dnorm(x, 1, 2), col="dodgerblue", lwd=2, xlim=c(-3,5), main="Normal distribution") # < 0: not good
curve(dgamma(x, 4, 2), col="dodgerblue", lwd=2, xlim=c(0,5), main="Gamma distribution") # [0, +\infty): good

```

It is important to note that the $\lambda$ parameter must represent lengths of phonecalls, which are some kind of measurement for time (a real value always greater than 0). The distribution of interest is a $\mathrm{Poisson}(\lambda)$, in which $\lambda$ is both the mean and the variance of the distribution.

The first proposed distribution is a $\mathrm{Beta}(4, 2)$. Such distribution bounds the $\lambda$ parameter between 0 and 1: this is not a desirable or realistic behavior, since -- in principle -- a phone call can have arbitrary length. The prior is thus formally correct but not realistically usable.

The second proposed prior is a Normal. As such, it is able to model negative values for $\lambda$, which is a non-physical behaviour of time in such context.

The only suitable prior distribution for $\lambda$ is the Gamma. It returns positive non-zero samples of $\lambda$, which corresponds to its natural domain.

Posterior can be computed with $\mathsf{Stan}$. The first step is to define the model:

```
data{
  int N;
  int y[N];
  real<lower=0> alpha;
  real<lower=0> beta;
}
parameters{
  real<lower=0> lambda;
}
model{
  target += poisson_lpmf( y | lambda );
  target += gamma_lpdf(lambda|alpha, beta);
}
```
With the model ready, posterior can be computed.

In order to fit it, 15 samples have been generated by using a $\mathrm{Poisson}(\lambda=4)$.

```{r}
# simulate the data
set.seed(123)
n <- 15
true_mean <- 4
y <- rpois(n,true_mean)

# prior params
alpha<-4
beta<-2

# fit Stan model
data<- list(N=n, y=y, alpha=alpha, beta=beta)
fit2 <- rstan::stan(file="./src/gamma_lambda.stan", data = data, chains = 4, iter=2000, refresh=-1);

# extract Stan output
sim <- rstan::extract(fit2)
sample_lambda <- sim[["lambda"]]

# posterior params
alpha_star = alpha + length(y)*mean(y)
beta_star = beta+length(y)

# plot
par(mfrow=c(1,1), pty ="m", oma=c(0,0,0,0))

hist(sim[["lambda"]], probability = TRUE, xlim=c(0,7), ylim=c(0,1.0), breaks=15, 
     col="darkmagenta", border="white",
     xlab=expression(lambda), ylab="Probability", main="Prior and posterior distribution")

curve(dgamma(x, 4, 2), col="dodgerblue", lwd=2,
      cex.lab=2, add=T)
curve(dgamma(x, alpha_star, beta_star), col="maroon", lwd=2,
      cex.lab=2, add=T)
abline(v=true_mean, col="darkgoldenrod1", lwd=3, lty=1)

legend(0, 1, c("Posterior", "Th. Posterior", "Prior", expression(paste("True ", lambda))), 
       c("darkmagenta", "maroon", "dodgerblue", "darkgoldenrod1"), cex=0.8)
```

The posterior distribution exhibits a maximum near to the true value of $\lambda$. The particular prior distribution and likelihood used in the model allow to compute also analitically the posterior distribution.

As it will be shown, the prior is the conjugate of the posterior. In the computation, multiplicative constants have been omitted. The first step is to write the likelihood and the prior a more friendly way:

$$
p(\mathbf{y}|\lambda)  =  \prod_{i=1}^N p(y_i|\lambda)  
              =  \prod_{i=1}^N \frac{e^{-\lambda} \lambda^{y_i}}{y_i!} 
              \propto e^{-N\lambda}\lambda^{N \bar{y}}; \ \ \ \
p(\lambda)  =  \frac{\beta^\alpha \lambda^{\alpha-1}  e^{-\beta \lambda}}{\Gamma(\alpha)}
            \propto  \lambda^{\alpha-1} e^{-\beta \lambda}
$$

Now both (simplified) expressions are multiplied.


$$
p(\lambda | \mathbf{y}) \ = \ p(\mathbf{y} | \lambda)p(\lambda)
                        \ \propto \ \lambda^{\alpha-1} e^{-\beta \lambda} e^{-N \lambda} \lambda^{N\bar{y}}
                         =  \lambda^{\alpha-1+N \bar{y}} e^{-\lambda (\beta + N)}
$$

In this expression, constants can be replaced as follows:
$$
\begin{align}
\alpha' &= \alpha + N \bar{y} \\
\beta' &= \beta + N
\end{align}
$$
The resulting expression then is the folowing:
$$
p(\lambda | \mathbf{y}) \ \propto \ \lambda^{\alpha'-1} e^{-\lambda \beta'} 
$$

This expression corresponds to the functional form of a $\mathrm{Gamma}(\alpha', \beta')$ distribution except for a multiplicative constant, which can be also seen as a normalization constant. Since the quantity $p(\lambda | \mathbf{y})$ must be necessarily a probability distribution, the only possible choice for the unknown normalization constant is to be the one of the Gamma distribution.

The complete posterior distribution is thus:

$$
p(\lambda | \mathbf{y}) = \mathrm{Gamma}(\alpha', \beta'); \ \ \ \ 
\alpha' = \alpha + N \bar{y}, \ \ \beta' = \beta + N
$$

Since such analytical expression of the posterior is known, it is easy to check the goodness of the MCMC-produced samples with a *qqplot*:

```{r}
# qqplot
n <- 100
p <- (1:n)/n

q_theor <- qgamma(p = p, alpha_star, beta_star)
q_empir <- quantile(sample_lambda, probs = p)

par(mfrow=c(1,1), pty ="m", oma=c(0,0,0,0))

plot(x=q_theor,
     y=q_empir,
     type="p",
     col="black", 
     main = "Gamma q-q plot",
     xlab="Theoretical quantiles",
     ylab="Empirical quantiles")

segments(x0=0, y0=0, x1=5.5, y1=5.5, col="red") 
```

The MCMC samples fit well in the theoretical, in a similar way as samples of the exercise 5.


## Exercise 8

### Text:

Go to this link: [rstan](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started), and follow the instructions to download and install the rstan library. Once you did it succesfully, open the file model called biparametric.stan, and replace the line

$$ \mathsf{ target+=cauchy\_lpdf(sigma| 0, 2.5);} $$ 
**with the following one:**

$$ \mathsf{ target+=uniform_lpdf(sigma|0.1,10);} $$
Which prior are you now assuming for your parameter $\sigma$? Reproduce the same plots as above and briefly comment.

### Solution:

The `biparametric.stan` file used has the following structure:
```
data{
  int N;
  real y[N];
  real a;
  real b;
}
parameters{
  real theta;
  real<lower=0> sigma;
}
model{
  target+=normal_lpdf(y|theta, sigma);
  target+=uniform_lpdf(theta|a, b );
  target+=uniform_lpdf(sigma|0.1,10);
}
```
In this case we're assuming that the parameter $\sigma$ has a prior distribution that is Uniform in the interval $[0.1,10]$
```{r echo=TRUE,  message=FALSE, warning=FALSE}
#true mean
theta_sample <- 2
#likelihood variance
sigma2 <- 2
#sample size
n <- 10

#generate some data
set.seed(123)
y <- rnorm(n,theta_sample, sqrt(sigma2))
```

```{r echo=TRUE,  message=FALSE, warning=FALSE}
library(rstan)
library(bayesplot)
library(rstanarm)
library(ggplot2)

# Make Stan go parallel!
options(mc.cores = parallel::detectCores())

# Avoid losing precious time
rstan::rstan_options(auto_write = TRUE)


data3<- list(N=n, y=y, a=-10, b=10)
fit3 <- rstan::stan(file="./src/biparametric.stan", data = data3, chains = 4, iter=2000,
             refresh=-1)
#extract stan output for biparametric model
sim3 <- rstan::extract(fit3)
posterior_biv <- as.matrix(fit3)
theta_est <- mean(sim3$theta)
sigma_est <- mean(sim3$sigma)
#mean of the samples
c(theta_est, sigma_est)
```

Once we have the MCMC samples, we represent the result with `mcmc_areas`: 
```{r echo=TRUE,  message=FALSE, warning=FALSE}
plot_title <- ggplot2::ggtitle("Posterior distributions", "with medians and 80% intervals")
bayesplot::mcmc_areas(posterior_biv, pars = c("theta","sigma"), prob = 0.8) + plot_title
```

From the result of the MCMC sampling we observe that mean and variance results for both $\theta$ and $\sigma$ are slightly different from the case in wich $\sigma \sim \text{Cauchy}^+ (0,2.5)$. This result suggests that priors used don't provide much information to the posterior distributions.

## Exercise 9

### Text:

Reproduce the first plot above for the soccer goals, but this time by replacing Prior 1 with a $\mathsf{Gamma}(2,4)$. Then, compute the final Bayes factor matrix ($\mathsf{BF\_matrix}$) with this new prior and the other ones unchanged, and comment. Is still Prior 2 favorable over all the others?

### Solution:

In this exercise we are interested in assessing the average number of goals scored by a given team in Major league Soccerr, and denote the goals for $n$ games as $y_1,…,y_n$.
```{r echo=TRUE,  message=FALSE, warning=FALSE}
library(LearnBayes)
data(soccergoals)

y <- soccergoals$goals
```
So we assume a poisson likelihood. Then we have to elicit a prior for the parameter $\mathsf{\lambda}$, with several plausible choices.\
We'll use the same code provided by the lab exercitation, but this time we'll change the first prior to compare in a $\mathsf{Gamma}(2,4)$ distribution.
```{r echo=TRUE,  message=FALSE, warning=FALSE}
#write the likelihood function via the gamma distribution

lik_pois<- function(data, theta){
  n <- length(data)
  lambda <- exp(theta)
  dgamma(lambda, shape =sum(data)+1, scale=1/n)
}

prior_gamma <- function(par, theta){
  lambda <- exp(theta)
  dgamma(lambda, par[1], rate=par[2])*lambda  
}

prior_norm <- function(npar, theta){
  lambda=exp(theta)  
  (dnorm(theta, npar[1], npar[2]))
  
}

lik_pois_v <- Vectorize(lik_pois, "theta")
prior_gamma_v <- Vectorize(prior_gamma, "theta")
prior_norm_v <- Vectorize(prior_norm, "theta")


#likelihood
curve(lik_pois_v(theta=x, data=y), xlim=c(-3,4), xlab=expression(theta), ylab = "log-scale density", lwd =2 )
#prior 1
curve(prior_gamma_v(theta=x, par=(c(2, 4))), lty =2, col="red", add = TRUE, lwd =2)
#prior 2 
curve(prior_norm_v(theta=x, npar=c(1, 0.5)), lty =3, col="blue", add =TRUE, lwd=2)
#prior 3 
curve(prior_norm_v(theta=x, npar=c(2, 0.5)), lty =4, col="green", add =TRUE, lwd =2)
#prior 4 
curve(prior_norm_v(theta=x, npar=c(1, 2)), lty =5, col="violet", add =TRUE, lwd =2)
legend(2.6, 1.8, c("Lik.", "Ga(4.57,1.43)", "N(1, 0.25)", "N(2,0.25)","N(1, 4)" ),
       lty=c(1,2,3,4,5), col=c("black", "red", "blue", "green", "violet"),lwd=2, cex=0.9)
```
As we can already see from this plot in log scale, the new prior we implemented (red) is pretty much extreme as the third prior. Their mean in log scale differ considerably from the likelihood one.  

Now we compute and display BF matrix:
```{r echo=TRUE,  message=FALSE, warning=FALSE}
logpoissongamma <- function(theta, datapar){
  data <- datapar$data
  par <- datapar$par
  lambda <- exp(theta)
  log_lik <- log(lik_pois(data, theta))
  log_prior <- log(prior_gamma(par, theta))
  return(log_lik+log_prior)
}

logpoissonnormal <- function( theta, datapar){
  data <- datapar$data
  npar <- datapar$par
  lambda <- exp(theta)
  log_lik <- log(lik_pois(data, theta))
  log_prior <- log(prior_norm(npar, theta))
  return(log_lik+log_prior)
}  

datapar <- list(data=y, par=c(2, 4))
fit1 <- laplace(logpoissongamma, .5, datapar)
datapar <- list(data=y, par=c(1, .5))
fit2 <- laplace(logpoissonnormal, .5, datapar)
datapar <- list(data=y, par=c(2, .5))
fit3 <- laplace(logpoissonnormal, .5, datapar)
datapar <- list(data=y, par=c(1, 2))
fit4 <- laplace(logpoissonnormal, .5, datapar)

logmarg <- c(fit1$int, fit2$int, fit3$int, fit4$int)

BF_matrix <- matrix(1, 4,4)
for (i in 1:3){
  for (j in 2:4){
    BF_matrix[i,j]<- exp(logmarg[i]-logmarg[j])
    BF_matrix[j,i]=(1/BF_matrix[i,j]) 
  }
}

round_bf <- round(BF_matrix,3)
round_bf
```
From the BF_matrix we notice that Prior 1 is worse than Prior 4 and Prior 2 as we expected, but thanks to the matrix we can better see that Prior 1 is better than Prior 3.
However Prior 2 is still favored over the other priors. 
## Exercise 10

### Text:

Let $y=(1,\ 0,\ 0,\ 1,\ 0,\ 0,\ 0,\ 0,\ 0,\ 1,\ 0,\ 0,\ 1,\ 0)$ collect the results of tossing $n=14$ times an unfair coin, where $1$ denotes $heads$ and $0$ $tails$, and $p=Prob(y_i=1)$.

* Looking at the $\mathsf{Stan}$ code for the other models, write a short $\mathsf{Stan}$ Beta-Binomial model, where $p$ has a $\mathsf{Beta}(a,b)$ prior with $a=3$,$b=3$.
* extract the posterior distribution with the function $\mathsf{extract}()$;
* produce some plots with the $\mathsf{bayesplot}$ package and comment.
* compute analitically the posterior distribution and compare it with the $\mathsf{Stan}$ distribution.

### Solution

First of all we load data,
```{r echo=TRUE,  message=FALSE, warning=FALSE}
library(rstan)
library(ggplot2)
library(bayesplot)

y <- c(1,0,0,1,0,0,0,0,0,1,0,0,1,0)
n <- 14
a <- 3
b <- 3
```
the `ex10.stan` file used has the following structure:
```
data{
  int N;
  int y[N];
  real alpha;
  real beta;
}
parameters{
  real<lower=0,upper=1> p;
}
model{
  target+=bernoulli_lpmf( y | p );
  target+=beta_lpdf( p | alpha, beta );
}
```
at this point we launch the Stan model:
```{r echo=TRUE,  message=FALSE, warning=FALSE}
data<- list(N=n, y=y, alpha=a , beta=b)
fit <- rstan::stan(file="./src/ex10.stan", data = data, chains = 4, iter=2000)
```
we extract the posterior distribution:
```{r echo=TRUE,  message=FALSE, warning=FALSE}
#extract Stan output
sim <- rstan::extract(fit)
```
and with bayesplot we analyse the results of the model.
```{r echo=TRUE,  message=FALSE, warning=FALSE}
# traceplot
traceplot(fit, pars ="p")


#MCMC areas
posterior <- as.matrix(fit)

plot_title <- ggplot2::ggtitle("Posterior distributions",
                      "with medians and 80% intervals")

bayesplot::mcmc_areas(posterior, 
           pars = "p", 
           prob = 0.8) + plot_title
```
The first plot gives us a visual representation of the sampling behavior for the four chains we use to produce the stan fit.
The second plot is the density plot computed from posterior draws with all the chains merged.
In the end we compute analytically the posterior distribution
$$
\mathsf{\pi}(p|y) \propto \mathsf{L}(p;y)\mathsf{\pi}(p) \\
\mathsf{\pi}(p|y) \propto \mathsf{Binom}(y;n,p)\mathsf{Beta}(a,b) \\
\mathsf{\pi}(p|y) \propto p^y(1-p)^{n-y}p^{a-1}(1-p)^{b-1} = p^{y+a-1}(1-p)^{n-y+b-1} \\
\mathsf{\pi}(p|y) \propto \mathsf{Beta}(a+y,b+n-y)
$$
In our specific case we have:
$$
\mathsf{\pi}(p|y) \propto \mathsf{Beta}(7,13)
$$
Which gives us
$$
\mathsf{E}[p] = \frac{a}{a+b} = 0.35 \\
\mathsf{Var}[p] = \frac{ab}{(a+b)^2(a+b+1)} = 0.01
$$
Finally we plot and compare the true analytical posterior and the Stan simulated posterior
```{r echo=TRUE,  message=FALSE, warning=FALSE}
par(mfrow=c(1,2), pty ="m", oma=c(0,0,0,0))

curve(dbeta(x, 3, 3),xlim=c(-1,2), lty=1, lwd=2, col="red", ylim=c(0,4), 
      ylab="density", xlab=expression(p), cex.lab=2)
lines(density(sim$p, adj=2), col ="blue", lwd=2, lty =1)
legend(0.5, 4, c("Prior", "Stan Posterior"),
       c("red", "blue" ), lty=c(1,1),lwd=c(1,1), cex=0.5)


curve(dbeta(x, 3, 3),xlim=c(-1,2), lty=1, lwd=2, col="red", ylim=c(0,4), 
      ylab="density", xlab=expression(p), cex.lab=2)
curve(dbeta(x, 7, 13), 
      xlab=expression(theta), ylab="", col="blue", lwd=2,
      cex.lab=2, add=T)  
legend(0.5, 4, c("Prior", "Analytical Posterior"), 
       c("red", "blue" ), lty=c(1,1),lwd=c(1,1), cex=0.5)
```
and we also compare them in the same plot
```{r echo=TRUE,  message=FALSE, warning=FALSE}
par(mfrow=c(1,1), pty ="m", oma=c(0,0,0,0))


curve(dbeta(x, 7, 13),xlim=c(-1,2), lty=2, lwd=1, col="black", ylim=c(0,4), 
      ylab="density", xlab=expression(theta), cex.lab=2)
curve(dbeta(x, 3, 3 ), xlim=c(-1,2), col="red", lty=1,lwd=2,  add =T)
lines(density(sim$p, adj=2), col ="blue", lwd=2, lty =1)
legend(0.5, 4, c("Prior", "Analytical posterior", "Stan Posterior"),
       c("red", "black", "blue", "blue" ), lty=c(1,2,1),lwd=c(1,1,2), cex=0.8)
```
We can barely see the difference between the Stan posterior and the analytical one.
